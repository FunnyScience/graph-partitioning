{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCOTCH Environment valid.\n",
      "SCOTCH python bindings were loaded correctly from ../csap-graphpartitioning/src/python \n",
      "SCOTCH Library was located successfully at ../csap-graphpartitioning/tools/scotch/lib/macOS/libscotch.dylib\n",
      "Graph loaded...\n",
      "Nodes: 1000\n",
      "Edges: 2939\n",
      "Graph is undirected\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shared\n",
    "import networkx as nx\n",
    "\n",
    "try:\n",
    "    import config\n",
    "except ImportError as err:\n",
    "    print(err)\n",
    "    print(\"**Could not load config.py\\n**Copy config_template.py and rename it.\")\n",
    "    #exit()\n",
    "\n",
    "pwd = %pwd\n",
    "\n",
    "DATA_FILENAME = os.path.join(pwd, \"data\", \"oneshot_fennel_weights.txt\")\n",
    "OUTPUT_DIRECTORY = os.path.join(pwd, \"output\")\n",
    "\n",
    "# Read input file for prediction model, if not provided a prediction\n",
    "# model is made using FENNEL\n",
    "PREDICTION_MODEL = \"\"\n",
    "\n",
    "# File containing simulated arrivals. This is used in simulating nodes\n",
    "# arriving at the shelter. Nodes represented by line number; value of\n",
    "# 1 represents a node as arrived; value of 0 represents the node as not\n",
    "# arrived or needing a shelter.\n",
    "SIMULATED_ARRIVAL_FILE = os.path.join(pwd, \"data\", \"simulated_arrival.txt\")\n",
    "#SIMULATED_ARRIVAL_FILE = \"\"\n",
    "\n",
    "# File containing the geographic location of each node.\n",
    "POPULATION_LOCATION_FILE = os.path.join(pwd, \"data\", \"population_location.csv\")\n",
    "\n",
    "# Number of shelters\n",
    "num_partitions = 4\n",
    "\n",
    "# The number of iterations when making prediction model\n",
    "num_iterations = 10\n",
    "\n",
    "# Percentage of prediction model to use before discarding\n",
    "# When set to 0, prediction model is discarded, useful for one-shot\n",
    "prediction_model_cut_off = 0.10\n",
    "\n",
    "# Alpha value used in one-shot (when restream_batches set to 1)\n",
    "one_shot_alpha = 0.5\n",
    "\n",
    "# Number of arrivals to batch before recalculating alpha and restreaming.\n",
    "# When set to 1, one-shot is used with alpha value from above\n",
    "restream_batches = 10\n",
    "\n",
    "# Create virtual nodes based on prediction model\n",
    "use_virtual_nodes = False\n",
    "\n",
    "# Virtual nodes: edge weight\n",
    "virtual_edge_weight = 1.0\n",
    "\n",
    "\n",
    "####\n",
    "# GRAPH MODIFICATION FUNCTIONS\n",
    "\n",
    "# Also enables the edge calculation function.\n",
    "graph_modification_functions = True\n",
    "\n",
    "# If set, the node weight is set to 100 if the node arrives at the shelter,\n",
    "# otherwise the node is removed from the graph.\n",
    "alter_arrived_node_weight_to_100 = True\n",
    "\n",
    "# Uses generalized additive models from R to generate prediction of nodes not\n",
    "# arrived. This sets the node weight on unarrived nodes the the prediction\n",
    "# given by a GAM.\n",
    "# Needs POPULATION_LOCATION_FILE to be set.\n",
    "alter_node_weight_to_gam_prediction = True\n",
    "\n",
    "gam_k_value = 100\n",
    "\n",
    "# Alter the edge weight for nodes that haven't arrived. This is a way to\n",
    "# de-emphasise the prediction model for the unknown nodes.\n",
    "prediction_model_emphasis = 1.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# read METIS file\n",
    "G = shared.read_metis(DATA_FILENAME)\n",
    "\n",
    "# Alpha value used in prediction model\n",
    "prediction_model_alpha = G.number_of_edges() * (num_partitions / G.number_of_nodes()**2)\n",
    "\n",
    "# Order of nodes arriving\n",
    "arrival_order = list(range(0, G.number_of_nodes()))\n",
    "\n",
    "# Arrival order should not be shuffled if using GAM to alter node weights\n",
    "#random.shuffle(arrival_order)\n",
    "\n",
    "if SIMULATED_ARRIVAL_FILE == \"\":\n",
    "    # mark all nodes as needing a shelter\n",
    "    simulated_arrival_list = [1]*G.number_of_nodes()\n",
    "else:\n",
    "    with open(SIMULATED_ARRIVAL_FILE, \"r\") as ar:\n",
    "        simulated_arrival_list = [int(line.rstrip('\\n')) for line in ar]\n",
    "\n",
    "print(\"Graph loaded...\")\n",
    "print(\"Nodes: {}\".format(G.number_of_nodes()))\n",
    "print(\"Edges: {}\".format(G.number_of_edges()))\n",
    "if nx.is_directed(G):\n",
    "    print(\"Graph is directed\")\n",
    "else:\n",
    "    print(\"Graph is undirected\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION MODEL\n",
      "----------------\n",
      "Using: SCOTCH Partitioning\n",
      "--------------------------\n",
      "\n",
      "WASTE\t\tCUT RATIO\tEDGES CUT\tCOMM VOLUME\n",
      "0.10000\t\t0.0309629126\t91\t\t127\n",
      "\n",
      "Assignments:\n",
      "[ 3  0  1  3  3  3  2  1  2  0  0  0  2  0  0  0  1  1  2  3  0  1  2  1  0  1  1  1  2  0  2  1  3  0  3  3  0  1  1  2  1  0  2  1  0  1  0  1  1  1  2  0  0  1  1  1  3  3  0  3  1  2  0  1  0  2  0  1  0  2  2  3  3  0  2  1  2  1  2  3  1  3  3  1  0  3  0  2  3  3  2  3  3  3  3  1  3  2  0  1  0  2  1  3  2  0  1  3  0  3  1  2  1  3  3  0  3  0  3  1  0  0  2  3  2  0  3  1  2  2  2  2  2  2  3  3  2  1  3  0  2  1  1  2  1  1  1  1  0  2  3  2  2  2  0  1  0  1  2  3  0  0  3  2  0  0  3  3  2  3  2  2  1  3  1  2  0  2  2  0  1  2  0  1  1  0  1  0  3  1  3  1  0  3  2  2  3  2  0  2  1  0  1  0  2  1  1  0  2  1  1  2  0  3  3  3  1  2  1  1  0  3  0  0  1  1  1  2  2  2  3  1  0  0  0  3  0  3  2  0  1  1  2  3  3  1  1  2  2  0  0  3  3  1  3  3  0  1  1  2  3  1  3  1  3  0  3  1  1  3  1  2  0  1  3  1  0  2  1  0  1  1  0  1  0  0  2  1  2  2  2  0  2  0  1  2  1  3  2  0  2  2  3  1  0  2  0  1  0  0  1  1  3  1  0  2  0  0  1  0  2  3  2  0  1  2  0  0  2  2  1  1  0  2  3  3  3  0  0  3  0  0  1  1  3  1  2  2  3  1  2  1  3  0  1  3  2  1  3  3  2  1  1  3  3  2  2  0  3  1  2  3  2  0  1  3  3  1  2  3  2  1  0  3  3  3  1  3  1  0  0  0  1  2  1  3  2  3  3  2  0  0  2  3  3  3  3  1  2  1  0  3  0  2  3  3  1  1  2  3  0  3  3  3  0  2  2  3  0  3  1  2  0  2  0  1  3  3  3  0  3  3  1  1  3  1  3  3  0  1  3  0  1  0  1  2  1  1  2  1  3  0  0  3  1  1  2  3  2  2  1  0  0  2  0  1  3  3  1  1  2  1  2  3  2  3  0  0  3  0  0  2  2  1  2  1  0  0  3  1  2  1  0  0  3  0  3  0  2  2  3  1  2  0  3  3  0  1  0  0  1  2  2  3  0  1  1  1  3  0  0  1  1  1  0  0  3  3  0  1  2  1  0  1  1  1  1  0  1  2  3  3  0  2  1  2  0  2  1  1  0  2  2  3  3  0  2  1  2  3  1  1  3  2  0  2  1  2  1  2  2  0  1  2  1  1  1  2  1  3  1  1  1  0  1  1  0  1  3  2  1  0  2  3  2  3  3  0  1  0  0  0  3  2  1  2  2  2  3  0  2  3  1  3  1  1  0  2  1  0  2  0  1  1  3  3  0  0  0  3  2  3  1  2  0  3  0  0  1  2  3  1  0  2  1  3  1  0  1  0  0  1  3  2  3  0  2  0  3  1  2  2  0  0  3  3  3  1  2  3  1  0  1  3  0  2  1  1  0  3  0  3  1  1  1  1  3  2  3  3  0  2  2  2  3  0  0  0  2  3  3  0  3  2  3  0  3  3  3  3  1  2  3  3  3  2  2  1  1  3  2  0  1  2  2  3  1  1  1  3  1  1  1  3  0  0  2  0  2  3  0  1  0  0  3  0  1  1  0  3  3  1  3  0  3  2  0  2  3  1  1  0  3  1  0  3  0  0  0  2  0  2  2  3  2  3  3  1  2  2  1  2  3  1  1  3  0  0  0  3  0  1  1  0  3  3  2  1  2  1  1  2  1  0  0  0  3  3  0  0  3  0  2  0  0  2  3  3  1  0  1  1  2  1  0  3  1  3  0  1  0  2  2  1  3  2  0  0  3  1  1  2  2  1  0  1  3  3  0  1  2  2  3  2  0  2  0  1  0  3  2  3  3  2  0  3  3  2  0  0  1  3  1  3  0  3  1  3  1  1  2  0  1  0  2  3  3  3  0  0  1  1  2  0  1  2  3  1  0  3  0  1  2  2  0  0  1  1  2  1  2  2  3  1  3  1  3  1  1  0  3  0  2  1  2  2  3  0  3  1  0  1  1  1  2  3  2  3  0  1  0  1  1  2  0  1  0  2  2  3  1  0  2  0  2  0  0  1  2  2  3  0  1  3  0  0  2  2  1  2  2  2  1  0  0  0  1  2  0  0  2  1  2  2  2  0  1  2  3  2]\n",
      "\n",
      "Fixed: 0\n",
      "\n",
      "Partitions - nodes (weight):\n",
      "P0: 253.0 (253)\n",
      "P1: 275.0 (275)\n",
      "P2: 232.0 (232)\n",
      "P3: 240.0 (240)\n"
     ]
    }
   ],
   "source": [
    "import pyximport; pyximport.install()\n",
    "import fennel\n",
    "import numpy as np\n",
    "\n",
    "# setup for other algorithms\n",
    "if config.ENABLE_SCOTCH == True:\n",
    "    # import the relevant SCOTCH modules\n",
    "    from scotch.graph_mapper import GraphMapper\n",
    "    from scotch.io import ScotchGraphArrays\n",
    "\n",
    "UNMAPPED = -1\n",
    "\n",
    "# reset\n",
    "assignments = np.repeat(np.int32(UNMAPPED), G.number_of_nodes())\n",
    "fixed = np.repeat(np.int32(UNMAPPED), G.number_of_nodes())\n",
    "\n",
    "print(\"PREDICTION MODEL\")\n",
    "print(\"----------------\")\n",
    "\n",
    "# Display which algorithm is being run\n",
    "if config.PREDICTION_MODEL_ALGORITHM == config.Partitioners.FENNEL:\n",
    "    print(\"Using: FENNEL Partitioning\")\n",
    "    print(\"---------------\\n\")\n",
    "elif config.PREDICTION_MODEL_ALGORITHM == config.Partitioners.SCOTCH:\n",
    "    print(\"Using: SCOTCH Partitioning\")\n",
    "    print(\"--------------------------\\n\")\n",
    "    \n",
    "predictionModels = {}\n",
    "# store model data for different types of partitioners\n",
    "# NOTE: THIS IS NOT IMPLEMENTED YET - need to discuss first\n",
    "if config.RUN_ALL_PREDICTION_MODEL_ALGORITHMS == True:\n",
    "    # create different prediction models\n",
    "    fennelModel = {}\n",
    "    fennelModel['assignments'] = np.repeat(np.int32(UNMAPPED), G.number_of_nodes())\n",
    "    fennelModel['fixed'] = np.repeat(np.int32(UNMAPPED), G.number_of_nodes())\n",
    "\n",
    "    predictionModels[config.Partitioners.FENNEL] = fennelModel\n",
    "    \n",
    "    scotchModel = {}\n",
    "    scotchModel['assignments'] = np.repeat(np.int32(UNMAPPED), G.number_of_nodes())\n",
    "    scotchModel['fixed'] = np.repeat(np.int32(UNMAPPED), G.number_of_nodes())\n",
    "\n",
    "    predictionModels[config.Partitioners.SCOTCH] = scotchModel\n",
    "    \n",
    "# Begin computation of prediction model\n",
    "if PREDICTION_MODEL:\n",
    "    # if we have a prediction model from file, load it\n",
    "    with open(PREDICTION_MODEL, \"r\") as inf:\n",
    "        assignments = np.fromiter(inf.readlines(), dtype=np.int32)\n",
    "\n",
    "else:\n",
    "    # choose the right algorithm\n",
    "    if config.PREDICTION_MODEL_ALGORITHM == config.Partitioners.FENNEL:\n",
    "        assignments = fennel.generate_prediction_model(G, num_iterations, num_partitions, assignments, fixed, prediction_model_alpha)\n",
    "    elif config.PREDICTION_MODEL_ALGORITHM == config.Partitioners.SCOTCH:\n",
    "        # SCOTCH algorithm\n",
    "        # we have a networkx graph already, G\n",
    "        scotchArrays = ScotchGraphArrays() # create the object storing all the SCOTCH arrays\n",
    "        scotchArrays.fromNetworkxGraph(G, baseval=0) # populate arrays from G\n",
    "        \n",
    "        #scotchArrays.debugPrint() # uncomment this to print out contents of scotchArrays\n",
    "        \n",
    "        # create instance of SCOTCH Library\n",
    "        mapper = GraphMapper(config.SCOTCH_LIB_PATH)\n",
    "\n",
    "        # set some optional parameters for the SCOTCH_Arch, SCOTCH_Strat, SCOTCH_Graph\n",
    "        # see csap-graphpartitioning/src/python/scotch/graph_mapper: GraphMapper.__init__() method for more options\n",
    "        mapper.kbalval = 0.1\n",
    "        mapper.numPartitions = num_partitions\n",
    "        \n",
    "        # intializes the SCOTCH_Arch, SCOTCH_Strat, SCOTCH_Graph using scotchArray and optional parameters\n",
    "        ok = mapper.initialize(scotchArrays, verbose=False)\n",
    "        if(ok):\n",
    "            # we can proceed with graphMap, the data structures were setup correctly\n",
    "            ok = mapper.graphMap()\n",
    "            if(ok):\n",
    "                # graphMap was run successfully, copy the assignments\n",
    "                # make a deep copy as we then delete the mapper data, to clear memory\n",
    "                # and the array reference may be lost\n",
    "                assignments = np.array(mapper.scotchData._parttab, copy=True)\n",
    "                \n",
    "                mapper.delObjects()\n",
    "            else:\n",
    "                print('Error while running graphMap()')\n",
    "        else:\n",
    "            print('Error while setting up SCOTCH for partitioning.')\n",
    "        \n",
    "x = shared.score(G, assignments, num_partitions)\n",
    "edges_cut, steps = shared.base_metrics(G, assignments)\n",
    "print(\"WASTE\\t\\tCUT RATIO\\tEDGES CUT\\tCOMM VOLUME\")\n",
    "print(\"{0:.5f}\\t\\t{1:.10f}\\t{2}\\t\\t{3}\".format(x[0], x[1], edges_cut, steps))\n",
    "\n",
    "print(\"\\nAssignments:\")\n",
    "shared.fixed_width_print(assignments)\n",
    "\n",
    "nodes_fixed = len([o for o in fixed if o == 1])\n",
    "print(\"\\nFixed: {}\".format(nodes_fixed))\n",
    "\n",
    "shared.print_partitions(G, assignments, num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if use_virtual_nodes:\n",
    "    print(\"Creating virtual nodes and assigning edges based on prediction model\")\n",
    "\n",
    "    # create virtual nodes\n",
    "    virtual_nodes = list(range(G.number_of_nodes(), G.number_of_nodes() + num_partitions))\n",
    "    print(\"\\nVirtual nodes:\")\n",
    "\n",
    "    # create virtual edges\n",
    "    virtual_edges = []\n",
    "    for n in range(0, G.number_of_nodes()):\n",
    "        virtual_edges += [(n, virtual_nodes[assignments[n]])]\n",
    "\n",
    "    # extend assignments\n",
    "    assignments = np.append(assignments, np.array(list(range(0, num_partitions)), dtype=np.int32))\n",
    "    fixed = np.append(fixed, np.array([1] * num_partitions, dtype=np.int32))\n",
    "\n",
    "    G.add_nodes_from(virtual_nodes, weight=1)\n",
    "    G.add_edges_from(virtual_edges, weight=virtual_edge_weight)\n",
    "\n",
    "    print(\"\\nAssignments:\")\n",
    "    shared.fixed_width_print(assignments)\n",
    "    print(\"Last {} nodes are virtual nodes.\".format(num_partitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assign first 100 arrivals using prediction model, then discard\n",
      "\n",
      "WASTE\t\tCUT RATIO\tEDGES CUT\tCOMM VOLUME\n",
      "8.00000\t\t0.1687648860\t496\t\t459\n",
      "\n",
      "Assignments:\n",
      "[-1  1  2  0 -1  0  0 -1 -1 -1  3 -1  0  1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1  1 -1 -1 -1  0 -1 -1  2 -1  3 -1 -1  1 -1 -1  1 -1  1 -1  1 -1  2  1 -1  3  2  0 -1 -1 -1 -1  2  0 -1  1  0 -1 -1 -1 -1  1 -1  3  2  1  3  1 -1 -1 -1 -1 -1  0 -1 -1  0 -1 -1 -1 -1  1 -1  1 -1 -1  0  0  3 -1  0  0 -1 -1 -1  1 -1  1 -1 -1 -1 -1  1 -1  0 -1  3 -1  0 -1 -1 -1  1  3  1 -1 -1  1 -1  0  0 -1  3 -1 -1  0  1  0 -1  1 -1 -1 -1  1 -1  3  3  1  2  2 -1 -1  1 -1  2 -1 -1 -1 -1  1  0  1  2  1 -1 -1 -1  1  1  0 -1  1  3  1  3 -1 -1 -1  1 -1  3 -1  1 -1 -1 -1  1 -1  0 -1 -1  3  1 -1  1 -1 -1 -1 -1 -1 -1  0  1 -1 -1  1  0 -1 -1 -1  1  0 -1 -1  1 -1 -1 -1 -1  3  0 -1  0 -1  0 -1 -1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "\n",
      "Fixed: 100\n",
      "\n",
      "Partitions - nodes:\n",
      "P0: 29\n",
      "P1: 45\n",
      "P2: 10\n",
      "P3: 16\n"
     ]
    }
   ],
   "source": [
    "cut_off_value = int(prediction_model_cut_off * G.number_of_nodes())\n",
    "if prediction_model_cut_off == 0:\n",
    "    print(\"Discarding prediction model\\n\")\n",
    "else:\n",
    "    print(\"Assign first {} arrivals using prediction model, then discard\\n\".format(cut_off_value))\n",
    "\n",
    "# fix arrivals\n",
    "nodes_arrived = []\n",
    "for a in arrival_order:\n",
    "    # check if node needs a shelter\n",
    "    if simulated_arrival_list[a] == 0:\n",
    "        continue\n",
    "\n",
    "    # set 100% node weight for those that need a shelter\n",
    "    if alter_arrived_node_weight_to_100:\n",
    "        G.node[a]['weight'] = 100\n",
    "\n",
    "    nodes_fixed = len([o for o in fixed if o == 1])\n",
    "    if nodes_fixed >= cut_off_value:\n",
    "        break\n",
    "    fixed[a] = 1\n",
    "    nodes_arrived.append(a)\n",
    "\n",
    "# remove nodes not fixed, ie. discard prediction model\n",
    "for i in range(0, len(assignments)):\n",
    "    if fixed[i] == -1:\n",
    "        assignments[i] = -1\n",
    "\n",
    "x = shared.score(G, assignments, num_partitions)\n",
    "edges_cut, steps = shared.base_metrics(G, assignments)\n",
    "print(\"WASTE\\t\\tCUT RATIO\\tEDGES CUT\\tCOMM VOLUME\")\n",
    "print(\"{0:.5f}\\t\\t{1:.10f}\\t{2}\\t\\t{3}\".format(x[0], x[1], edges_cut, steps))\n",
    "\n",
    "print(\"\\nAssignments:\")\n",
    "shared.fixed_width_print(assignments)\n",
    "\n",
    "nodes_fixed = len([o for o in fixed if o == 1])\n",
    "print(\"\\nFixed: {}\".format(nodes_fixed))\n",
    "\n",
    "shared.print_partitions(G, assignments, num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning in batches of 10\n",
      "--------------------------------\n",
      "\n",
      "WASTE\t\tCUT RATIO\tEDGES CUT\tCOMM VOLUME\tALPHA\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'rpy2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0f2f5c705621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgam_k_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mgam_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgam_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPOPULATION_LOCATION_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_arrived\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/voreno/Development/CSAP/graph-partitioning/shared.py\u001b[0m in \u001b[0;36mgam_predict\u001b[0;34m(population_csv, num_arrived, k_value)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrpy2_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobjects\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFormula\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'rpy2'"
     ]
    }
   ],
   "source": [
    "if restream_batches == 1:\n",
    "    print(\"One-shot assignment mode\")\n",
    "    print(\"------------------------\\n\")\n",
    "else:\n",
    "    print(\"Assigning in batches of {}\".format(restream_batches))\n",
    "    print(\"--------------------------------\\n\")\n",
    "\n",
    "def edge_expansion(G):\n",
    "    # Update edge weights for nodes that have an assigned probability of displacement\n",
    "    for edge in G.edges_iter(data=True):\n",
    "        left = edge[0]\n",
    "        right = edge[1]\n",
    "        edge_weight = edge[2]['weight_orig']\n",
    "\n",
    "        # new edge weight\n",
    "        edge[2]['weight'] = (float(G.node[left]['weight']) * edge_weight) * (float(G.node[right]['weight']) * edge_weight)\n",
    "\n",
    "        if left in nodes_arrived or right in nodes_arrived:\n",
    "            # change the emphasis of the prediction model\n",
    "            edge[2]['weight'] = edge[2]['weight'] * prediction_model_emphasis\n",
    "\n",
    "    return G\n",
    "\n",
    "# preserve original node/edge weight\n",
    "if graph_modification_functions:\n",
    "    node_weights = {n[0]: n[1]['weight'] for n in G.nodes_iter(data=True)}\n",
    "    nx.set_node_attributes(G, 'weight_orig', node_weights)\n",
    "\n",
    "    edge_weights = {(e[0], e[1]): e[2]['weight'] for e in G.edges_iter(data=True)}\n",
    "    nx.set_edge_attributes(G, 'weight_orig', edge_weights)\n",
    "\n",
    "\n",
    "# SETUP SCOTCH VARIABLES\n",
    "scotchMapper = None\n",
    "scotchArrayData = None\n",
    "if config.ASSIGNMENT_MODEL_ALGORITHM == config.Partitioners.SCOTCH:\n",
    "    scotchMapper = GraphMapper(config.SCOTCH_LIB_PATH, numPartitions=num_partitions)\n",
    "    scotchArrayData = ScotchGraphArrays()\n",
    "    \n",
    "batch_arrived = []\n",
    "print(\"WASTE\\t\\tCUT RATIO\\tEDGES CUT\\tCOMM VOLUME\\tALPHA\")\n",
    "for i, a in enumerate(arrival_order):\n",
    "\n",
    "    # check if node is already arrived\n",
    "    if fixed[a] == 1:\n",
    "        continue\n",
    "\n",
    "    # GRAPH MODIFICATION FUNCTIONS\n",
    "    if graph_modification_functions:\n",
    "\n",
    "        # remove nodes that don't need a shelter\n",
    "        if simulated_arrival_list[a] == 0:\n",
    "            G.remove_node(a)\n",
    "            continue\n",
    "        \n",
    "        # set 100% node weight for those that need a shelter\n",
    "        if alter_arrived_node_weight_to_100:\n",
    "            G.node[a]['weight'] = 100\n",
    "\n",
    "    # one-shot assigment: assign each node as it arrives\n",
    "    if restream_batches == 1:\n",
    "        alpha = one_shot_alpha\n",
    "        \n",
    "        if config.ASSIGNMENT_MODEL_ALGORITHM == config.Partitioners.FENNEL:\n",
    "            partition_votes = fennel.get_votes(G, a, num_partitions, assignments)\n",
    "            assignments[a] = fennel.get_assignment(G, a, num_partitions, assignments, partition_votes, alpha, 0)\n",
    "        elif config.ASSIGNMENT_MODEL_ALGORITHM == config.Partitioners.SCOTCH:\n",
    "            # load array data from graph\n",
    "            scotchArrayData.fromNetworkxGraph(G, parttab=assignments)\n",
    "            ok = scotchMapper.initialize(scotchArrayData)\n",
    "            if(ok):\n",
    "                # mapper initialized\n",
    "                ok = scotchMapper.graphMapFixed()\n",
    "                if(ok):\n",
    "                    assignments = scotchMapper.scotchData._parttab\n",
    "                else:\n",
    "                    print(\"Error running graphMapFixed()\")\n",
    "            else:\n",
    "                print(\"Error initializing SCOTCH GraphMapper for graphMapFixed()\")\n",
    "        fixed[a] = 1\n",
    "        nodes_arrived.append(a)\n",
    "\n",
    "        # make a subgraph of all arrived nodes\n",
    "        Gsub = G.subgraph(nodes_arrived)\n",
    "\n",
    "        x = shared.score(Gsub, assignments, num_partitions)\n",
    "        edges_cut, steps = shared.base_metrics(Gsub, assignments)\n",
    "        print(\"{0:.5f}\\t\\t{1:.10f}\\t{2}\\t\\t{3}\\t\\t{4:.10f}\".format(x[0], x[1], edges_cut, steps, alpha))\n",
    "        continue\n",
    "\n",
    "    batch_arrived.append(a)\n",
    "\n",
    "    if restream_batches == len(batch_arrived) or i == len(arrival_order) - 1:\n",
    "\n",
    "        # GRAPH MODIFICATION FUNCTIONS\n",
    "        if graph_modification_functions:\n",
    "\n",
    "            # set node weight to prediction generated from a GAM\n",
    "            if alter_node_weight_to_gam_prediction:\n",
    "                total_arrived = nodes_arrived + batch_arrived + [a]\n",
    "                if len(total_arrived) < gam_k_value:\n",
    "                    k = len(total_arrived)\n",
    "                else:\n",
    "                    k = gam_k_value\n",
    "\n",
    "                gam_weights = shared.gam_predict(POPULATION_LOCATION_FILE, len(total_arrived), k)\n",
    "\n",
    "                for node in G.nodes_iter():\n",
    "                    if alter_arrived_node_weight_to_100 and node in total_arrived:\n",
    "                        pass # weight would have been set previously\n",
    "                    else:\n",
    "                        G.node[node]['weight'] = int(gam_weights[node] * 100)\n",
    "\n",
    "            G = edge_expansion(G)\n",
    "\n",
    "        # make a subgraph of all arrived nodes\n",
    "        Gsub = G.subgraph(nodes_arrived + batch_arrived)\n",
    "\n",
    "        # recalculate alpha\n",
    "        if Gsub.is_directed():\n",
    "            # as it's a directed graph, edges_arrived is actually double, so divide by 2\n",
    "            edges_arrived = Gsub.number_of_edges() / 2\n",
    "        else:\n",
    "            edges_arrived = Gsub.number_of_edges()\n",
    "        nodes_fixed = len([o for o in fixed if o == 1])\n",
    "        alpha = (edges_arrived) * (num_partitions / (nodes_fixed + len(batch_arrived))**2)\n",
    "\n",
    "        if alter_node_weight_to_gam_prediction:\n",
    "            # justification: the gam learns the entire population, so run fennal on entire population\n",
    "            assignments = fennel.generate_prediction_model(G,\n",
    "                                                           num_iterations,\n",
    "                                                           num_partitions,\n",
    "                                                           assignments,\n",
    "                                                           fixed,\n",
    "                                                           alpha)\n",
    "        else:\n",
    "            # use the information we have, those that arrived\n",
    "            assignments = fennel.generate_prediction_model(Gsub,\n",
    "                                                           num_iterations,\n",
    "                                                           num_partitions,\n",
    "                                                           assignments,\n",
    "                                                           fixed,\n",
    "                                                           alpha)\n",
    "\n",
    "\n",
    "        # assign nodes to prediction model\n",
    "        for n in batch_arrived:\n",
    "            fixed[n] = 1\n",
    "            nodes_arrived.append(n)\n",
    "\n",
    "        x = shared.score(Gsub, assignments, num_partitions)\n",
    "        edges_cut, steps = shared.base_metrics(Gsub, assignments)\n",
    "        print(\"{0:.5f}\\t\\t{1:.10f}\\t{2}\\t\\t{3}\\t\\t{4:.10f}\".format(x[0], x[1], edges_cut, steps, alpha))\n",
    "        batch_arrived = []\n",
    "\n",
    "# remove nodes not fixed\n",
    "for i in range(0, len(assignments)):\n",
    "    if fixed[i] == -1:\n",
    "        assignments[i] = -1\n",
    "\n",
    "print(\"\\nAssignments:\")\n",
    "shared.fixed_width_print(assignments)\n",
    "\n",
    "nodes_fixed = len([o for o in fixed if o == 1])\n",
    "print(\"\\nFixed: {}\".format(nodes_fixed))\n",
    "\n",
    "shared.print_partitions(G, assignments, num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if use_virtual_nodes:\n",
    "    print(\"Remove virtual nodes\")\n",
    "    \n",
    "    print(\"\\nCurrent graph:\")\n",
    "    print(\"Nodes: {}\".format(G.number_of_nodes()))\n",
    "    print(\"Edges: {}\".format(G.number_of_edges()))\n",
    "\n",
    "    G.remove_nodes_from(virtual_nodes)\n",
    "    assignments = np.delete(assignments, virtual_nodes)\n",
    "    fixed = np.delete(fixed, virtual_nodes)\n",
    "\n",
    "    print(\"\\nVirtual nodes removed:\")\n",
    "    print(\"Nodes: {}\".format(G.number_of_nodes()))\n",
    "    print(\"Edges: {}\".format(G.number_of_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add partition attribute to nodes\n",
    "for i in range(0, len(assignments)):\n",
    "    G.add_nodes_from([i], partition=str(assignments[i]))\n",
    "\n",
    "# Remove original node/edge weights\n",
    "for node in G.nodes_iter(data=True):\n",
    "    if 'weight_orig' in node[1]:\n",
    "        del node[1]['weight_orig']\n",
    "for edge in G.edges_iter(data=True):\n",
    "    if 'weight_orig' in edge[2]:\n",
    "        del edge[2]['weight_orig']\n",
    "\n",
    "# Freeze Graph from further modification\n",
    "G = nx.freeze(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%H%M%S')\n",
    "data_filename,_ = os.path.splitext(os.path.basename(DATA_FILENAME))\n",
    "data_filename += \"-\" + timestamp\n",
    "\n",
    "graph_metrics = {\n",
    "    \"file\": timestamp,\n",
    "    \"num_partitions\": num_partitions,\n",
    "    \"num_iterations\": num_iterations,\n",
    "    \"prediction_model_cut_off\": prediction_model_cut_off,\n",
    "    \"one_shot_alpha\": one_shot_alpha,\n",
    "    \"restream_batches\": restream_batches,\n",
    "    \"use_virtual_nodes\": use_virtual_nodes,\n",
    "    \"virtual_edge_weight\": virtual_edge_weight,\n",
    "}\n",
    "graph_fieldnames = [\n",
    "    \"file\",\n",
    "    \"num_partitions\",\n",
    "    \"num_iterations\",\n",
    "    \"prediction_model_cut_off\",\n",
    "    \"one_shot_alpha\",\n",
    "    \"restream_batches\",\n",
    "    \"use_virtual_nodes\",\n",
    "    \"virtual_edge_weight\",\n",
    "    \"edges_cut\",\n",
    "    \"waste\",\n",
    "    \"cut_ratio\",\n",
    "    \"communication_volume\",\n",
    "    \"network_permanence\",\n",
    "    \"Q\",\n",
    "    \"NQ\",\n",
    "    \"Qds\",\n",
    "    \"intraEdges\",\n",
    "    \"interEdges\",\n",
    "    \"intraDensity\",\n",
    "    \"modularity degree\",\n",
    "    \"conductance\",\n",
    "    \"expansion\",\n",
    "    \"contraction\",\n",
    "    \"fitness\",\n",
    "    \"QovL\",\n",
    "]\n",
    "\n",
    "print(\"Complete graph with {} nodes\".format(G.number_of_nodes()))\n",
    "(file_maxperm, file_oslom) = shared.write_graph_files(OUTPUT_DIRECTORY, \"{}-all\".format(data_filename), G)\n",
    "\n",
    "# original scoring algorithm\n",
    "scoring = shared.score(G, assignments, num_partitions)\n",
    "graph_metrics.update({\n",
    "    \"waste\": scoring[0],\n",
    "    \"cut_ratio\": scoring[1],\n",
    "})\n",
    "\n",
    "# edges cut and communication volume\n",
    "edges_cut, steps = shared.base_metrics(G)\n",
    "graph_metrics.update({\n",
    "    \"edges_cut\": edges_cut,\n",
    "    \"communication_volume\": steps,\n",
    "})\n",
    "\n",
    "# MaxPerm\n",
    "max_perm = shared.run_max_perm(file_maxperm)\n",
    "graph_metrics.update({\"network_permanence\": max_perm})\n",
    "\n",
    "# Community Quality metrics\n",
    "community_metrics = shared.run_community_metrics(OUTPUT_DIRECTORY,\n",
    "                                                 \"{}-all\".format(data_filename),\n",
    "                                                 file_oslom)\n",
    "graph_metrics.update(community_metrics)\n",
    "\n",
    "print(\"\\nConfig\")\n",
    "print(\"-------\\n\")\n",
    "for f in graph_fieldnames[:8]:\n",
    "    print(\"{}: {}\".format(f, graph_metrics[f]))\n",
    "\n",
    "print(\"\\nMetrics\")\n",
    "print(\"-------\\n\")\n",
    "for f in graph_fieldnames[8:]:\n",
    "    print(\"{}: {}\".format(f, graph_metrics[f]))\n",
    "\n",
    "# write metrics to CSV\n",
    "metrics_filename = os.path.join(OUTPUT_DIRECTORY, \"metrics.csv\")\n",
    "shared.write_metrics_csv(metrics_filename, graph_fieldnames, graph_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partition_metrics = {}\n",
    "partition_fieldnames = [\n",
    "    \"file\",\n",
    "    \"partition\",\n",
    "    \"network_permanence\",\n",
    "    \"Q\",\n",
    "    \"NQ\",\n",
    "    \"Qds\",\n",
    "    \"intraEdges\",\n",
    "    \"interEdges\",\n",
    "    \"intraDensity\",\n",
    "    \"modularity degree\",\n",
    "    \"conductance\",\n",
    "    \"expansion\",\n",
    "    \"contraction\",\n",
    "    \"fitness\",\n",
    "    \"QovL\",\n",
    "]\n",
    "\n",
    "for p in range(0, num_partitions):\n",
    "    partition_metrics = {\n",
    "        \"file\": timestamp,\n",
    "        \"partition\": p\n",
    "    }\n",
    "\n",
    "    nodes = [i for i,x in enumerate(assignments) if x == p]\n",
    "    Gsub = G.subgraph(nodes)\n",
    "    print(\"\\nPartition {} with {} nodes\".format(p, Gsub.number_of_nodes()))\n",
    "    print(\"-----------------------------\\n\")\n",
    "\n",
    "    (file_maxperm, file_oslom) = shared.write_graph_files(OUTPUT_DIRECTORY, \"{}-p{}\".format(data_filename, p), Gsub)\n",
    "    \n",
    "    # MaxPerm\n",
    "    max_perm = shared.run_max_perm(file_maxperm)\n",
    "    partition_metrics.update({\"network_permanence\": max_perm})\n",
    "\n",
    "    # Community Quality metrics\n",
    "    community_metrics = shared.run_community_metrics(OUTPUT_DIRECTORY,\n",
    "                                                     \"{}-p{}\".format(data_filename, p),\n",
    "                                                     file_oslom)\n",
    "    partition_metrics.update(community_metrics)\n",
    "\n",
    "    print(\"\\nMetrics\")\n",
    "    for f in partition_fieldnames:\n",
    "        print(\"{}: {}\".format(f, partition_metrics[f]))\n",
    "\n",
    "    # write metrics to CSV\n",
    "    metrics_filename = os.path.join(OUTPUT_DIRECTORY, \"metrics-partitions.csv\")\n",
    "    shared.write_metrics_csv(metrics_filename, partition_fieldnames, partition_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
